import os
import pickle
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from keras.models import load_model
from multi_classify_model.model import CBRNN, my_LSTM
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from keras.models import Model
from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.callbacks import EarlyStopping
from common.read_feature_from_db import act_dataset_api_calls_to_ids
from sklearn.model_selection import train_test_split
from common.sklearn_classify import print_value
from sklearn.model_selection import train_test_split
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras import layers
from keras.models import Model
from keras.layers import add, multiply, concatenate, Input, Embedding, Conv1D, Bidirectional, GRU, LSTM, \
    BatchNormalization, MaxPooling1D, GlobalMaxPooling1D, Dense, Flatten
import matplotlib.pyplot as plt
import numpy as np
import os
from sklearn.metrics import classification_report, accuracy_score, recall_score, roc_curve, auc, f1_score, \
    precision_score

from common.get_kaggle_data_and_label import get_x_and_y
from common.mongo_func import read_from_mongodb_ObjectId, read_from_mongodb, write_to_mongodb


def train(model, model_name, x_train, y_train, x_val, y_val, max_len=500, batch_size=64, verbose=True, epochs=10,
          save_path='../saved/',
          save_best=True):
    # callbacks
    print("epochs=", epochs)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, mode='auto')
    ear = EarlyStopping(monitor='val_acc', patience=5)
    mcp = ModelCheckpoint(os.path.join(save_path, model_name),
                          monitor="val_acc",
                          save_best_only=save_best,
                          save_weights_only=False)
    history = model.fit(
        x_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[ear, mcp, reduce_lr],
        validation_data=(x_val, y_val))
    model.save(model_name)
    return model, history


def run():
    max_len = 500
    word_num = 370
    # word_num = 16
    for i in range(1):
        print('loading data ... ...')
        x, y, full_id_list = act_dataset_api_calls_to_ids(500, need_api_id_seq=False, need_type_id_seq=True, sorted=True)

        # X_train_val, X_test, y_train_val, y_test = train_test_split(x, y, test_size=0.2, random_state=42,
        #                                                             shuffle=True)
        # dummy_predict=[2]*len(y_test)
        # print_value(y_test, dummy_predict, "dummy_predict")
        # return

        # x, y = get_x_and_y(balanced=True)
        ohe = OneHotEncoder()
        y = ohe.fit_transform(y).toarray()
        X_train_val, X_test, y_train_val, y_test = train_test_split(x, y, test_size=0.2, random_state=42,
                                                                    shuffle=True)  # ,stratify=y)
        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42,
                                                          shuffle=True)
        print("train:", len(y_train))
        print("test:", len(y_test))
        print("val:", len(y_val))

        dropout = 0.5
        recurrent_dropout = 0.5
        model_name = 'id_and_type_textCNN_only.h5'
        # model=my_LSTM(max_len,word_num)
        model = CBRNN(max_len, vocab_size=word_num, dropout=dropout, recurrent_dropout=recurrent_dropout)
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
        model.summary()
        print('dropout=%f recurrent_dropout=%f' % (dropout, recurrent_dropout))
        print('%s Train on %d data, test on %d data' % (model_name, len(X_train), len(X_test)))
        model, history = train(model=model, model_name=model_name, x_train=X_train, y_train=y_train, x_val=X_val,
                               y_val=y_val, max_len=max_len, batch_size=32, epochs=10)
        # model = load_model(os.path.join('../saved/', model_name))
        test_mse_score, test_mae_score = model.evaluate(X_test, y_test)
        print(test_mse_score, test_mae_score)

        test_pre = model.predict(X_test)
        print_value(np.argmax(y_test, axis=1), np.argmax(test_pre, axis=1), "sandboxdata_full_my_model")


def draw_auc(y_test, pred):
    fpr, tpr, thresholds = roc_curve(y_test, pred, pos_label=1, drop_intermediate=True)
    plt.plot([0, 1], [0, 1], color='darkorange', linestyle='--')

    plt.plot(fpr, tpr, marker='o')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic Curve')
    plt.legend(loc="lower right")
    plt.show()
    plt.savefig('../trans/auc_balanced.png')
    print(thresholds)
    AUC = auc(fpr, tpr)
    print(AUC)


def load_model_to_test(model_name):
    print("load model ", model_name)
    model = load_model(model_name)
    x, y = get_x_and_y(balanced=True)
    # x, y, full_id_list = act_dataset_api_calls_to_ids(500, need_api_id_seq=True, need_type_id_seq=True, sorted=True)

    ohe = OneHotEncoder()
    y = ohe.fit_transform(y).toarray()

    X_train_val, X_test, y_train_val, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)

    test_pre = model.predict(X_test)
    # y_test_list = []
    # test_pre_list = []
    # for i in range(len(y_test)):
    #     if y_test[i][0] > y_test[i][1]:
    #         y_test_list.append(0)
    #     else:
    #         y_test_list.append(1)
    #
    #     test_pre_list.append(test_pre[i][1])
    # draw_auc(y_test_list, test_pre_list)
    # print_value(y_test, test_pre, model_name)
    print_value(np.argmax(y_test, axis=1), np.argmax(test_pre, axis=1), model_name)


def get_full_dataset_and_train():
    x, y, full_id_list = act_dataset_api_calls_to_ids(500, need_api_id_seq=True, need_type_id_seq=True, sorted=True)
    print(len(x))
    print(len(y))
    print(len(full_id_list))
    id_index_dict = {}

    for i in range(len(full_id_list)):
        id_index_dict[full_id_list[i]] = i

    train_dataset_df = pd.read_csv("../stacking_model/train_dataset_ids.csv")
    test_dataset_df = pd.read_csv("../stacking_model/test_dataset_ids.csv")
    print(train_dataset_df.head(3))
    print(train_dataset_df.shape)
    print(test_dataset_df.head(3))
    print(test_dataset_df.shape)
    train_label_set = {}
    test_label_set = {}

    for index, row in train_dataset_df.iterrows():
        id = row['mongoid']
        train_label_set[id] = row['label_id']

    for index, row in test_dataset_df.iterrows():
        id = row['mongoid']
        test_label_set[id] = row['label_id']

    print("get all labels!")

    train_X = []
    train_y = []
    test_X = []
    test_y = []

    train_ids = []
    test_ids = []
    feature_num = 1000
    for item in train_label_set:
        train_ids.append(item)
        if item in id_index_dict:
            train_X.append(x[id_index_dict[item]])
        else:
            train_X.append([0] * feature_num)
        train_y.append(train_label_set[item])

    for item in test_label_set:
        test_ids.append(item)
        if item in id_index_dict:
            test_X.append(x[id_index_dict[item]])
        else:
            test_X.append([0] * feature_num)
        test_y.append(test_label_set[item])

    print(len(train_X), len(train_y), len(test_X), len(test_y))
    train_X = np.array(train_X)
    test_X = np.array(test_X)
    print("train_X.shape=", train_X.shape)
    print("test_X.shape=", test_X.shape)
    train_y = np.asarray(train_y).reshape((len(train_y), 1))
    test_y = np.asarray(test_y).reshape((len(test_y), 1))
    print("train_y:", train_y)
    ohe = OneHotEncoder()
    train_y = ohe.fit_transform(train_y).toarray()
    test_y = ohe.fit_transform(test_y).toarray()

    dropout = 0.5
    recurrent_dropout = 0.5
    max_len = 1000
    word_num = 370
    k_fold = 5
    batch = len(train_X) // k_fold
    print("x sum:", len(train_X), " batch num:", batch)
    last_end = 0
    train_proba = []
    mongoids = []
    test_proba = np.zeros((len(test_ids), 9))

    for i in range(k_fold):
        begin = last_end
        last_end = begin + batch
        if i == k_fold - 1:
            last_end = len(train_X)
        print("train batch no.", i, " valid from x[", begin, ":", last_end, "]")
        cur_train_test_X = train_X[begin:last_end]
        cur_train_test_y = train_y[begin:last_end]
        cur_mongo_id_list = train_ids[begin:last_end]
        print("len(cur_mongo_id_list)=", len(cur_mongo_id_list))
        combo_X = np.concatenate([train_X[:begin], train_X[last_end:]])
        combo_y = np.concatenate([train_y[:begin], train_y[last_end:]])
        print("combo_X.shape=", combo_X.shape)
        print("combo_y.shape=", combo_y.shape)

        print("data in train set")
        model_name = 'sandboxdata_full_11_21_kfold_' + str(i) + '.h5'
        model = CBRNN(max_len, vocab_size=word_num, dropout=dropout, recurrent_dropout=recurrent_dropout)
        model.summary()
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
        train_arr, val_arr, train_label, test_label = train_test_split(combo_X,
                                                                       combo_y, test_size=0.2, random_state=42,
                                                                       shuffle=True)
        model, history = train(model=model, model_name=model_name, x_train=train_arr, y_train=train_label,
                               x_val=val_arr,
                               y_val=test_label, max_len=max_len, batch_size=32, epochs=30)
        test_pre = model.predict(cur_train_test_X)
        print("test_pre[:2]", test_pre[:2])
        print_value(cur_train_test_y, test_pre, "api seq k fold")
        train_proba.extend(test_pre)
        mongoids.extend(cur_mongo_id_list)

        print("data in test set")
        predict_res = model.predict(test_X)
        arr = np.asarray(predict_res)
        test_proba += arr

    print("begging save to csv")
    df1 = pd.DataFrame(data=train_proba)
    df1['mongoid'] = mongoids
    print(df1.head(3))
    print(df1.shape)

    test_proba = test_proba / k_fold
    df2 = pd.DataFrame(test_proba)
    df2['mongoid'] = test_ids
    print(df2.head(3))
    print(df2.shape)

    df = pd.concat([df1, df2], axis=0, ignore_index=True)
    print("after concat ", df.shape)
    feature_prefix = "/home/dell/tmy_repos/malware_detection_code/stacking_model/predict_res/"
    df.to_csv(feature_prefix + "API_SEQ_predict.csv", index=False)


# nohup python3 -u  train_rnn.py  > nohup.log 2>&1 &
# export PYTHONPATH=/home/dell/tmy_repos/malware_detection_code/
run()


# load_model_to_test("/home/dell/tmy_repos/malware_detection_code/multi_classify_model/kaggle_balanced_data_9768.h5")
# load_model_to_test("/home/dell/tmy_repos/malware_detection_code/multi_classify_model/kaggle_balanced_data_11_28.h5") 96.29
# load_model_to_test("/home/dell/tmy_repos/malware_detection_code/multi_classify_model/kaggle_balanced.h5")

# run()
def get_opcode_word_bag():
    normal_dict = read_from_mongodb_ObjectId("192.168.105.224", "static_info_db", "opcode_normal")
    word_dict = {}
    no = 0
    for id in normal_dict:
        print(no, id)
        no += 1
        if 'opcodes' not in normal_dict[id]:
            continue
        for opcode in normal_dict[id]['opcodes']:
            if opcode in word_dict:
                word_dict[opcode] += 1
            else:
                word_dict[opcode] = 1
    print("count over")
    opcode_list = []
    frequency_list = []
    for item in word_dict:
        opcode_list.append(item)
        frequency_list.append(word_dict[item])
    df = pd.DataFrame(data={'opcode': opcode_list, 'num': frequency_list})
    df.to_csv("opcode_count.csv", index=False)
    print("saved!")


def opcode_to_list_and_saveDB():
    df = pd.read_csv("../multi_classify_model/opcode_count.csv")
    opcode_dict = {}
    id = 1
    for item in df['opcode'].tolist()[:200]:
        opcode_dict[item] = id
        id += 1
    to_insert = []
    normal_dict = read_from_mongodb_ObjectId("192.168.105.224", "static_info_db", "opcode_malware")
    no = 0
    for item in normal_dict:
        print(no, item)
        no += 1
        seq = normal_dict[item]['opcodes']
        pre = ''
        cur_list = []
        for code in seq:
            if code not in opcode_dict:
                continue
            if pre != code:
                cur_list.append(code)
                pre = code
        id_list = []
        for code in cur_list[:500]:
            id_list.append(opcode_dict[code])
        if len(id_list) < 500:
            id_list.extend([0] * (500 - len(id_list)))

        to_insert.append({'_id': item, 'opcodes': cur_list, 'seq': id_list})

    write_to_mongodb("192.168.105.224", "static_info_db", "malware_op_list", to_insert)


def get_opcodes_full_and_train():
    normal_dict = read_from_mongodb_ObjectId("192.168.105.224", "static_info_db", "normal_op_list")
    malware_dict = read_from_mongodb_ObjectId("192.168.105.224", "static_info_db", "malware_op_list")
    lookfor_op_mongoid = read_from_mongodb("192.168.105.224", "static_info_db", "filename_mongo_reportid_dict")
    train_dataset_df = pd.read_csv("../stacking_model/train_dataset_ids.csv")
    test_dataset_df = pd.read_csv("../stacking_model/test_dataset_ids.csv")
    print(train_dataset_df.head(3))
    print(train_dataset_df.shape)
    print(test_dataset_df.head(3))
    print(test_dataset_df.shape)

    train_X = []
    train_y = []
    test_X = []
    test_y = []

    train_ids = []
    test_ids = []
    feature_num = 500
    print("begin split train and test")
    for index, row in train_dataset_df.iterrows():
        id = row['mongoid']
        train_ids.append(id)
        find = False
        if id in lookfor_op_mongoid:
            opcode_mongoid = lookfor_op_mongoid[id]['opcode_mongoid']
            if opcode_mongoid in malware_dict:
                train_X.append(malware_dict[opcode_mongoid]['seq'])
                train_y.append(row['label_id'])
                find = True
            elif opcode_mongoid in normal_dict:
                train_X.append(normal_dict[opcode_mongoid]['seq'])
                train_y.append(row['label_id'])
                find = True
        if not find:
            train_X.append([0] * feature_num)
            train_y.append(row['label_id'])

    for index, row in test_dataset_df.iterrows():
        id = row['mongoid']
        test_ids.append(id)
        find = False
        if id in lookfor_op_mongoid:
            opcode_mongoid = lookfor_op_mongoid[id]['opcode_mongoid']
            if opcode_mongoid in malware_dict:
                test_X.append(malware_dict[opcode_mongoid]['seq'])
                test_y.append(row['label_id'])
                find = True
            elif opcode_mongoid in normal_dict:
                test_X.append(normal_dict[opcode_mongoid]['seq'])
                test_y.append(row['label_id'])
                find = True
        if not find:
            test_X.append([0] * feature_num)
            test_y.append(row['label_id'])

    print(len(train_y))
    print(len(test_y))
    dig_label = test_y.copy()
    print("get all labels!")

    print(len(train_X), len(train_y), len(test_X), len(test_y))
    train_X = np.array(train_X)
    test_X = np.array(test_X)
    print("train_X.shape=", train_X.shape)
    print("test_X.shape=", test_X.shape)
    train_y = np.asarray(train_y).reshape((len(train_y), 1))
    test_y = np.asarray(test_y).reshape((len(test_y), 1))
    print("train_y:", train_y)
    ohe = OneHotEncoder()
    train_y = ohe.fit_transform(train_y).toarray()
    test_y = ohe.fit_transform(test_y).toarray()

    dropout = 0.5
    recurrent_dropout = 0.5
    max_len = 500
    word_num = 210
    k_fold = 1
    batch = len(train_X) // k_fold
    print("x sum:", len(train_X), " batch num:", batch)
    last_end = 0
    train_proba = []
    mongoids = []
    test_proba = np.zeros((len(test_ids), 9))

    for i in range(k_fold):
        begin = last_end
        last_end = begin + batch
        if i == k_fold - 1:
            last_end = len(train_X)
        print("train batch no.", i, " valid from x[", begin, ":", last_end, "]")
        cur_train_test_X = train_X[begin:last_end]
        cur_train_test_y = train_y[begin:last_end]
        cur_mongo_id_list = train_ids[begin:last_end]
        print("len(cur_mongo_id_list)=", len(cur_mongo_id_list))
        if k_fold == 1:
            combo_X = np.asarray(train_X)
            combo_y = np.asarray(train_y)
        else:
            combo_X = np.concatenate([train_X[:begin], train_X[last_end:]])
            combo_y = np.concatenate([train_y[:begin], train_y[last_end:]])
        print("combo_X.shape=", combo_X.shape)
        print("combo_y.shape=", combo_y.shape)

        print("data in train set")
        model_name = 'opcode_single_model' + str(i) + '.h5'
        model = CBRNN(max_len, vocab_size=word_num, dropout=dropout, recurrent_dropout=recurrent_dropout)
        model.summary()
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
        train_arr, val_arr, train_label, test_label = train_test_split(combo_X,
                                                                       combo_y, test_size=0.2, random_state=42,
                                                                       shuffle=True)
        model, history = train(model=model, model_name=model_name, x_train=train_arr, y_train=train_label,
                               x_val=val_arr,
                               y_val=test_label, max_len=max_len, batch_size=32, epochs=2)
        test_pre = model.predict(cur_train_test_X)
        print("test_pre[:2]", test_pre[:2])
        print_value(cur_train_test_y, test_pre, "api seq k fold")
        train_proba.extend(test_pre)
        mongoids.extend(cur_mongo_id_list)

        print("data in test set")
        predict_res = model.predict(test_X)
        arr = np.asarray(predict_res)
        test_proba += arr
        print_value(np.argmax(predict_res, axis=1), dig_label, "opcode")

    # print("begging save to csv")
    # df1 = pd.DataFrame(data=train_proba)
    # df1['mongoid'] = mongoids
    # print(df1.head(3))
    # print(df1.shape)
    #
    # test_proba = test_proba / k_fold
    # df2 = pd.DataFrame(test_proba)
    # df2['mongoid'] = test_ids
    # print(df2.head(3))
    # print(df2.shape)
    #
    # df = pd.concat([df1, df2], axis=0, ignore_index=True)
    # print("after concat ", df.shape)
    # feature_prefix = "/home/dell/tmy_repos/malware_detection_code/stacking_model/predict_res/"
    # df.to_csv(feature_prefix + "OPCODE_SEQ.csv", index=False)

# get_full_dataset_and_train()
# get_opcodes_full_and_train()

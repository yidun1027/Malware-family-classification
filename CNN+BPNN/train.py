import os
import pickle
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from keras.models import load_model

from model import CBRNN
from data_preprocess import train_test_split, get_data, get_csv, get_calls, data_split


def train(model, model_name, max_len=500, batch_size=64, verbose=True, epochs=1000, save_path='../saved/', save_best=True):
    
    # callbacks
    reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.5, patience=2, mode='auto')
    ear = EarlyStopping(monitor='val_acc', patience=5)
    mcp = ModelCheckpoint(os.path.join(save_path, model_name), 
                          monitor="val_acc", 
                          save_best_only=save_best, 
                          save_weights_only=False)
    
    # history = model.fit_generator(
    #     data_generator(x_train, train_label, max_len, batch_size, shuffle=True),
    #     steps_per_epoch=len(x_train)//batch_size + 1,
    #     epochs=epochs, 
    #     callbacks=[ear, mcp],
    #     validation_data=data_generator(x_val, val_label, max_len, batch_size),
    #     validation_steps=len(x_val)//batch_size + 1)
    history = model.fit(
        x_train, y_train,
        epochs=epochs, 
        batch_size=batch_size,
        callbacks=[ear, mcp, reduce_lr],
        validation_data=(x_val, y_val))
    return model, history

    
if __name__ == '__main__':

    max_len = 0

    filename = 'dataset\\calls.csv'
    data_dir = 'dataset'
    datafname = os.path.join(data_dir, '1000_calls.txt')
    labelfname = os.path.join(data_dir, 'labels.txt')

    for i in range(10):
        print('loading data ... ...')
        max_len += 100
        # data, label = get_data(datafname, labelfname, max_len)
        # data, label = get_csv(filename)
        # print('get %d data,%d label' % (len(data), len(label)))
        data ,labels, word_num = get_calls(max_len)
        x_train, y_train, x_val, y_val, x_test, y_test = data_split(data, labels)
        # x_train, y_train, x_val, y_val, x_test, y_test = get_csv(filename)
        # x_train, y_train, x_val, y_val, x_test, y_test = train_test_split(data, label)
    
        dropout = 0.5
        recurrent_dropout = 0.5
        model_name = 'Malcbrnn' + str(max_len) + '.h5'
        model = CBRNN(max_len, vocab_size= word_num, dropout=dropout, recurrent_dropout=recurrent_dropout)
        model.summary()
    # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])
        print('dropout=%f recurrent_dropout=%f' % (dropout, recurrent_dropout))
        print('%s Train on %d data, test on %d data' % (model_name, len(x_train), len(x_test)))
        model, history = train(model=model, model_name=model_name, max_len=max_len, batch_size=32)
        model = load_model(os.path.join('../saved/', model_name))
        test_mse_score, test_mae_score = model.evaluate(x_test, y_test)
        print(test_mse_score, test_mae_score)
    # with open(os.path.join('../saved/', 'history.pkl'), 'wb') as f:
    #     pickle.dump(history.history, f)


# -*- coding:utf-8 -*-
from sklearn.feature_extraction.text import TfidfVectorizer
from common.mongo_func import write_to_mongodb
from pymongo import MongoClient
from common.static_import import read_word_dict
from common.mongo_func import write_to_mongodb
from bson.objectid import ObjectId


def extract():
    col_list = ["cuckoo_nfs_db", "cuckoo_nfs_db2", "cuckoo_nfs_db3", "cuckoo_nfs_db4",
                "cuckoo_nfs_db5", "cuckoo_nfs_db6", "cuckoo_nfs_db7", "cuckoo_nfs_db8"]
    normal_list = ["cuckoo_nfs_db6", "cuckoo_nfs_db7", "cuckoo_nfs_db8"]
    vec_dict = read_word_dict()
    print("vec_dict num:",len(vec_dict))
    client = MongoClient("192.168.105.224", 27017, unicode_decode_error_handler='ignore')
    database = client["db_imports"]
    for col in col_list:
        data_list = []
        collection = database[col]
        vec = [0] * (len(vec_dict))
        for line in collection.find():
            id = str(ObjectId(line['_id']))
            imports_list = str(line['imports']).split()
            for api in imports_list:
                if api in vec_dict:
                    vec[vec_dict[api]] = 1
            vec_str = ','.join([str(i) for i in vec])
            data_list.append({'_id': id, "import_vec": vec_str})

        if col not in normal_list:
            write_to_mongodb("192.168.105.224", "db_imports", "malware_vec", data_list)
        else:
            write_to_mongodb("192.168.105.224", "db_imports", "common_vec", data_list)
        print(col," done!",len(data_list))
        data_list.clear()
    client.close()


def extract2():
    col_list = ["cuckoo_nfs_db", "cuckoo_nfs_db2", "cuckoo_nfs_db3", "cuckoo_nfs_db4",
                "cuckoo_nfs_db5", "cuckoo_nfs_db6", "cuckoo_nfs_db7", "cuckoo_nfs_db8"]
    normal_list = ["cuckoo_nfs_db6", "cuckoo_nfs_db7", "cuckoo_nfs_db8"]
    client = MongoClient("192.168.105.224", 27017, unicode_decode_error_handler='ignore')
    database = client["db_imports"]
    import_list=[]
    for col in col_list:
        print("read ",col)
        collection = database[col]
        for line in collection.find():
            imports_str= str(line['imports'])
            import_list.append(imports_str)
    client.close()

    tfidf_model = TfidfVectorizer(max_df=0.5, min_df=1800)
    tfidf_matrix = tfidf_model.fit_transform(import_list)
    print(tfidf_matrix.shape)
    word_dict = tfidf_model.get_feature_names()

    print(len(word_dict))
    with open("imports_word_dict.txt", "w") as file:  # ”w"代表着每次运行都覆盖内容
        for word in word_dict:
            file.write(word + "\n")

extract()